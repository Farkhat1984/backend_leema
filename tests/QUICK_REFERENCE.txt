"""
Quick Test Reference - Common Commands
"""

# ============================================
# QUICK START
# ============================================

# Run all tests
./run_full_api_tests.sh
# or
python run_api_tests.py all

# ============================================
# SPECIFIC TEST CATEGORIES
# ============================================

# Authentication tests
python run_api_tests.py auth

# User operations
python run_api_tests.py user

# Shop operations
python run_api_tests.py shop

# Product management
python run_api_tests.py products

# Payment flows (PayPal)
python run_api_tests.py payments

# AI Generations (Gemini)
python run_api_tests.py generations

# Admin operations
python run_api_tests.py admin

# Real-world scenarios
python run_api_tests.py scenarios

# With coverage report
python run_api_tests.py coverage

# ============================================
# DIRECT PYTEST COMMANDS
# ============================================

# Run specific test class
pytest tests/test_full_api_integration.py::TestAuthenticationFlow -v

# Run specific test method
pytest tests/test_full_api_integration.py::TestAuthenticationFlow::test_create_test_token_user -v

# Run with markers
pytest -m integration -v
pytest -m payments -v
pytest -m generations -v
pytest -m admin -v

# Run with output
pytest tests/test_real_scenarios.py -v -s

# Stop on first failure
pytest tests/test_full_api_integration.py -x

# Show all test names without running
pytest tests/test_full_api_integration.py --collect-only

# ============================================
# TEST CREDENTIALS (from .env)
# ============================================

# User Account
Email: zfaragj@gmail.com
Role: User/Admin

# Shop Account  
Email: ckdshfh@gmail.com
Role: Shop Owner

# PayPal Sandbox
Test Email: sb-0qexx39406981@business.example.com
Test Pass: YPETl7&<
Mode: sandbox

# Gemini AI
API Key: From .env GEMINI_API_KEY
Model: gemini-2.0-flash-exp

# ============================================
# COVERAGE COMMANDS
# ============================================

# Generate HTML coverage
pytest tests/ --cov=app --cov-report=html
open htmlcov/index.html

# Show missing lines
pytest tests/ --cov=app --cov-report=term-missing

# Coverage for specific module
pytest tests/ --cov=app.api --cov-report=html

# ============================================
# DEBUGGING
# ============================================

# Show print statements
pytest tests/test_real_scenarios.py -v -s

# Show local variables on failure
pytest tests/test_full_api_integration.py -l

# Drop into debugger on failure
pytest tests/test_full_api_integration.py --pdb

# Verbose output
pytest tests/test_full_api_integration.py -vv

# ============================================
# FILTERING TESTS
# ============================================

# Run tests matching pattern
pytest tests/ -k "user" -v
pytest tests/ -k "payment" -v
pytest tests/ -k "generation" -v

# Run tests NOT matching pattern
pytest tests/ -k "not admin" -v

# ============================================
# PERFORMANCE
# ============================================

# Show slowest tests
pytest tests/ --durations=10

# Set timeout for slow tests
pytest tests/ --timeout=300

# ============================================
# CONTINUOUS INTEGRATION
# ============================================

# Run with strict markers
pytest tests/ --strict-markers

# Fail on first error
pytest tests/ -x

# Maximum failures before stopping
pytest tests/ --maxfail=5

# Parallel execution (requires pytest-xdist)
pytest tests/ -n auto

# ============================================
# TEST REPORTS
# ============================================

# Generate JUnit XML
pytest tests/ --junitxml=report.xml

# Generate HTML report
pytest tests/ --html=report.html --self-contained-html

# ============================================
# ENVIRONMENT CHECKS
# ============================================

# Check .env file
cat .env | grep -v '^#'

# Verify database
docker-compose ps postgres

# Test database connection
python -c "from app.database import engine; import asyncio; asyncio.run(engine.connect())"

# ============================================
# COMMON ISSUES & SOLUTIONS
# ============================================

# Issue: Tests fail with 401
# Solution: Check token generation in conftest.py

# Issue: Database connection error
# Solution: docker-compose up -d postgres

# Issue: PayPal tests timeout
# Solution: Check PAYPAL_CLIENT_ID in .env

# Issue: Gemini tests fail
# Solution: Verify GEMINI_API_KEY in .env

# Issue: Import errors
# Solution: pip install -r requirements.txt

# ============================================
# USEFUL PYTEST OPTIONS
# ============================================

# -v           verbose
# -s           show print statements
# -x           stop on first failure
# -k EXPR      only run tests matching expression
# -m MARKER    only run tests with marker
# --lf         run last failed tests
# --ff         run failures first, then others
# --tb=short   short traceback format
# --tb=line    one line per failure
# --collect-only   show tests without running

# ============================================
# TEST FILES
# ============================================

tests/
├── conftest.py                    # Fixtures & configuration
├── test_full_api_integration.py   # All endpoint tests (60+ tests)
├── test_real_scenarios.py         # User journey tests (8 scenarios)
├── TEST_GUIDE.md                  # Detailed guide
├── README_TESTS.md                # Quick reference
└── QUICK_REFERENCE.txt           # This file

# ============================================
# API ENDPOINT COVERAGE
# ============================================

Authentication:     8 tests  ✅
Users:              6 tests  ✅
Shops:              5 tests  ✅
Products:          10 tests  ✅
Reviews:            3 tests  ✅
Payments:           3 tests  ✅
Generations:        6 tests  ✅
Admin:             12 tests  ✅
Health:             3 tests  ✅
Scenarios:          8 tests  ✅
-----------------------------------
TOTAL:            64+ tests  ✅

# ============================================
# NEXT STEPS
# ============================================

1. Run: python run_api_tests.py all
2. Check output for failures
3. Review coverage report
4. Fix failing tests if any
5. Add new tests for new features
6. Keep documentation updated
